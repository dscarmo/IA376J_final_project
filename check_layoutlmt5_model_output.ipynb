{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import LayoutLMT5\n",
    "from metrics import compute_f1, compute_exact"
   ]
  },
  {
   "source": [
    "# T5 Base (without LayoutLM)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing T5...\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = LayoutLMT5.load_from_checkpoint(\"models/slower/slower-epoch=5.ckpt\", strict=False).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val DocVQA folder data/raw/val tokenizer PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}) transform None seq_len 512 no_image True\n",
      "prediction: 8.28\n",
      "target: 0.28\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: University of California, San Diego\n",
      "target: university of california\n",
      "F1: 0.5, Exact: 0\n",
      "\n",
      "prediction: ITC Limited\n",
      "target: itc limited\n",
      "F1: 1.0, Exact: 1\n",
      "\n",
      "prediction: CALIFORNIA, SAN DIEGO\n",
      "target: san diego\n",
      "F1: 0.8, Exact: 0\n",
      "\n",
      "prediction: Mr. Ms.\n",
      "target: Paul\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: 1128 SIXTEENTH ST., N.W., WASHINGTON, D. C. 20036\n",
      "target: 1128 SIXTEENTH ST., N. W., WASHINGTON, D. C. 20036\n",
      "F1: 0.823529411764706, Exact: 0\n",
      "\n",
      "prediction: AASHIRVAAD\n",
      "target: aashirvaad\n",
      "F1: 1.0, Exact: 1\n",
      "\n",
      "prediction: THE ROBERT A. WELCH FOUNDATION\n",
      "target: The Robert A. Welch Foundation\n",
      "F1: 1.0, Exact: 1\n",
      "\n",
      "prediction: 11:14 to 11:39 a.m.\n",
      "target: 11:14 to 11:39 a.m.\n",
      "F1: 1.0, Exact: 1\n",
      "\n",
      "prediction: $ 975.00\n",
      "target: $975.00\n",
      "F1: 0, Exact: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.hparams.bs = 10\n",
    "val_dataloader = model.val_dataloader()\n",
    "batch = next(iter(val_dataloader))\n",
    "cuda_batch = {}\n",
    "for k, v in batch.items():\n",
    "    if k in [\"document\", \"target_text\", \"input_text\"]:\n",
    "        cuda_batch[k] = v\n",
    "    else:\n",
    "        cuda_batch[k] = v.cuda()  \n",
    "batch = cuda_batch\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch)\n",
    "\n",
    "output_text = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "tgt_text = batch[\"target_text\"]\n",
    "for i in range(10):\n",
    "    print(f\"prediction: {output_text[i]}\\ntarget: {tgt_text[i]}\\nF1: {compute_f1(output_text[i], tgt_text[i])}, Exact: {compute_exact(output_text[i], tgt_text[i])}\\n\")"
   ]
  },
  {
   "source": [
    "# LayoutLMT5 model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing LayoutLM...\n",
      "Freezing LayoutLM...: 203it [00:00, 96415.32it/s]\n",
      "Initializing T5...\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = LayoutLMT5.load_from_checkpoint(\"models/frozen_llm/frozen_llm-epoch=8-val_loss=0.00-val_extact_match=0.00-val_f1=0.00.ckpt\", strict=False).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val DocVQA folder data/raw/val tokenizer PreTrainedTokenizer(name_or_path='microsoft/layoutlm-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}) transform None seq_len 512 no_image True\n",
      "prediction: $ 1, 000\n",
      "target: 0.28\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: university of california\n",
      "target: university of california\n",
      "F1: 1.0, Exact: 1\n",
      "\n",
      "prediction: rjr\n",
      "target: itc limited\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: new york\n",
      "target: san diego\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: dr. darby\n",
      "target: Paul\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: new york\n",
      "target: 1128 SIXTEENTH ST., N. W., WASHINGTON, D. C. 20036\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: rjr\n",
      "target: aashirvaad\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: dr. robert e. shank\n",
      "target: The Robert A. Welch Foundation\n",
      "F1: 0.22222222222222224, Exact: 0\n",
      "\n",
      "prediction: 12 : 00 noon\n",
      "target: 11:14 to 11:39 a.m.\n",
      "F1: 0, Exact: 0\n",
      "\n",
      "prediction: $ 1, 000. 00\n",
      "target: $975.00\n",
      "F1: 0, Exact: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.hparams.bs = 10\n",
    "val_dataloader = model.val_dataloader()\n",
    "batch = next(iter(val_dataloader))\n",
    "cuda_batch = {}\n",
    "for k, v in batch.items():\n",
    "    if k in [\"document\", \"target_text\", \"input_text\"]:\n",
    "        cuda_batch[k] = v\n",
    "    else:\n",
    "        cuda_batch[k] = v.cuda()  \n",
    "batch = cuda_batch\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch)\n",
    "\n",
    "output_text = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "tgt_text = batch[\"target_text\"]\n",
    "for i in range(10):\n",
    "    print(f\"prediction: {output_text[i]}\\ntarget: {tgt_text[i]}\\nF1: {compute_f1(output_text[i], tgt_text[i])}, Exact: {compute_exact(output_text[i], tgt_text[i])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}